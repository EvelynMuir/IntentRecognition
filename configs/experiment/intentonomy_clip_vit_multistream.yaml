# @package _global_

# to execute this experiment run:
# python train.py experiment=intentonomy_clip_vit_multistream

defaults:
  - override /data: intentonomy
  - override /model: intentonomy_clip_vit_multistream
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["intentonomy", "clip_vit", "multistream", "semantic_vq"]

callbacks:
  model_checkpoint:
    monitor: val/f1_macro
    mode: max

  early_stopping:
    monitor: val/f1_macro
    mode: max
    patience: 10

data:
  batch_size: 1024
  binarize_softprob: true

logger:
  wandb:
    name: "Intentonomy-CLIP-ViT-MultiStream"
    tags: ${tags}
    group: "Intentonomy-CLIP-ViT-MultiStream"

  tensorboard:
    name: "Intentonomy-CLIP-ViT-MultiStream"

# Model configuration overrides
model:
  semantic_anchors_path: "semantic_anchors.pth"  # Update this path as needed
  freeze_codebook: true  # Set to false for fine-tuning stage
  use_semantic_consistency_loss: false  # Set to true when freeze_codebook=false
  semantic_consistency_weight: 0.1
  
  # Learning rates for each VQ (can be adjusted)
  lr_vq_coco: 5e-3
  lr_vq_places: 5e-3
  lr_vq_emotion: 5e-3
  lr_vq_ava: 5e-3
  lr_vq_actions: 5e-3
  lr_classifier: 1e-2
  
  # Weight decay
  wd_vq: 0.0
  wd_head: 1e-4

  intent_loss_weight_warmup_epochs: 5
